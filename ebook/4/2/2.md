### 4.2.2 CYK Recognition with a Grammar in Chomsky Normal Form

Two of the restrictions that we want to impose on the grammar are obvious by now: no unit rules and no ε-rules. We would also like to limit the maximum length of a right-hand side to 2; this would reduce the time complexity to O(n3). It turns out that there is a form for CF grammars that exactly fits these restrictions: the Chomsky Normal Form. It is as if this normal form was invented for this algorithm. A grammar is in Chomsky Normal Form (CNF), when all rules either have the form A → a, or A → BC, where a is a terminal and A, B, and C are non-terminals. Fortunately, as we shall see later, any CF grammar can be mechanically transformed into a CNF grammar.

We will first discuss how the CYK algorithm works for a grammar in CNF. There are no ε-rules in a CNF grammar, so Rε is empty. The sets Ri,1 can be read directly from the rules: they are determined by the rules of the form A→a. A rule A→BC can never derive a single terminal, because there are no ε-rules.

Next, we proceed iteratively as before, first processing all substrings of length 2, then all substrings of length 3, etc. When a right-hand side BC is to derive a substring of length l, B has to derive the first part (which is non-empty), and C the rest (also non-empty).

![图1](../../img/4.2.2_1.png)

So B must derive si,k, that is, Bmust be a member of Ri,k, and likewise C must derive si+k,l−k; that is, C must be a member of Ri+k,l−k. Determining if such a k exists is easy: just try all possibilities; they range from 1 to l −1. All sets Ri,k and Ri+k,l−k have already been computed at this point.

This process is much less complicated than the one we saw before, which worked with a general CF grammar, for two reasons. The most important one is that we do not have to repeat the process again and again until no new non-terminals are added to Ri,l . Here, the substrings we are dealing with are really substrings: they cannot be equal to the string we started out with. The second reason is that we have to find only one place where the substring must be split in two, because the right-hand side consists of only two non-terminals. In ambiguous grammars, there can be several different splittings, but at this point that does not worry us. Ambiguity is a parsing issue, not a recognition issue.

The algorithm results in a complete collection of sets Ri,l . The sentence t consists of only n symbols, so a substring starting at position i can never have more than n+1−i symbols. This means that there are no substrings si,l with i+l > n+1. Therefore, the Ri,l sets can be organized in a triangular table, as depicted in Figure 4.8. This table is called the recognition table, or the well-formed substring table.

![图1](../../img/4.2.2_2-Fig.4.8.png)

The entry Ri,l is computed from entries along the arrows V and W simultaneously, as follows. The first entry we consider is Ri,1, at the start of arrow V. All non-terminals B in Ri,1 produce substrings which start at position i and have a length 1. Since we are trying to obtain parsings for the substring starting at position i with length l, we are now interested in substrings starting at i+1 and having length l−1. These should be looked for in Ri+1,l−1, at the start of arrow W. Now we combine each of the Bs found in Ri,1 with each of the Cs found in Ri+1,l−1, and for each pair B and C for which there is a rule A→BC in the grammar, we insert A in Ri,l . Likewise a B in Ri,2 can be combined into an A with a C from Ri+2,l−2, etc., and we continue in this way until we reach Ri,l−1 at the end point of V and Ri+l−1,1 at the end ofW.

The entry Ri,l cannot be computed until all entries below it are known in the triangle of which it is the top. This restricts the order in which the entries can be computed but still leaves some freedom. One way to compute the recognition table is depicted in Figure 4.9(a); it follows our earlier description in which no substring of length l is recognized until all string of length l−1 have been recognized. We could also compute the recognition table in the order depicted in Figure 4.9(b). In this order, Ri,l is computed as soon as all sets and input symbols needed for its computation are available. This order is particularly suitable for on-line parsing, where the number of symbols in the input is not known in advance, and additional information is computed each time a new symbol is read.

![图1](../../img/4.2.2_3-Fig.4.9.png)

Now let us examine the cost of this algorithm. Figure 4.8 shows that there are n(n+1)/2 entries to be filled. Filling in an entry requires examining all entries on the arrow V, of which there are at most n; usually there are fewer, and in practical situations many of the entries are empty and need not be examined at all. We will call the number of entries that really have to be considered nocc for “number of occurrences”; it is usually much smaller than n and for many grammars it is even a constant, but for worst-case estimates it should be replaced by n. Once the entry on the arrow v has been chosen, the corresponding entry on the arrowW is fixed, so the cost of finding it does not depend on n. As a result the algorithm has a time requirement of O(n2nocc) and operates in a time proportional to the cube of the length of the input sentence in the worst case, as already announced at the beginning of this section.

The cost of the algorithm also depends on the properties of the grammar. The entries along the V andW arrows can each contain at most |VN| non-terminals, where |VN| is the number of non-terminals in the grammar, the size of the set VN from the formal definition of a grammar in Section 2.2. But again the actual number is usually much lower, since usually only a very limited subset of the non-terminals can produce a segment of the input of a given length in a given position; we will indicate the number by |VN|occ. So the cost of one combination step is O(|VN|2 occ). Finding the rule in the grammar that combines B and C into an A can be done in constant time, by hashing or precomputation, and does not add to the cost of one combination step. This gives an overall time requirement of O(|VN|2 occn2nocc).

There is some disagreement in the literature over whether the second index of the recognition table should represent the length or the end position of the recognized segment. It is obvious that both carry the same information, but sometimes one is more convenient and at other times the other. There is some evidence, from Earley parsing (Section 7.2) and parsing as intersection (Chapter 13), that using the end point is more fundamental, but for CYK parsing the length is more convenient, both conceptually and for drawing pictures.